super_glue	record	['passage', 'query']
super_glue	multirc	['paragraph', 'question', 'answer']
super_glue	copa	['premise', 'choice1', 'choice2']
super_glue	axg	['premise', 'hypothesis']
super_glue	boolq	['passage', 'question']
super_glue	rte	['premise', 'hypothesis']
super_glue	wic	['word', 'sentence1', 'sentence2']
super_glue	axb	['sentence1', 'sentence2']
super_glue	wsc.fixed	['text', 'span1_text']
super_glue	cb	['premise', 'hypothesis']
emo		['text']
math_qa		['Problem', 'options', 'correct']
aqua_rat	raw	['question', 'correct']
anli		['premise', 'hypothesis']
kelm		['triple', 'sentence']
yelp_polarity		['text']
rotten_tomatoes		['text']
winograd_wsc	wsc273	['text', 'pronoun']
winograd_wsc	wsc285	['text', 'pronoun']
sick		['sentence_A', 'sentence_B']
mdd	task1_qa	[]
mdd	task3_qarecs	[]
mdd	task2_recs	[]
cbt	CN	[]
cbt	NE	[]
cbt	V	[]
cbt	raw	[]
cbt	P	[]
craigslist_bargains		[]
glue	cola	['sentence']
glue	sst2	['sentence']
glue	ax	['hypothesis', 'premise']
glue	stsb	['sentence1', 'sentence2']
glue	wnli	['sentence1', 'sentence2']
glue	mnli_mismatched	['premise', 'hypothesis']
glue	qqp	['question1', 'question2']
glue	mrpc	['sentence1', 'sentence2']
glue	mnli	['premise', 'hypothesis']
glue	mnli_matched	['premise', 'hypothesis']
glue	qnli	['sentence', 'question']
glue	rte	['sentence1', 'sentence2']
blended_skill_talk		['message_f', 'message_g']
snips_built_in_intents		['text']
numer_sense		['target']
cosmos_qa		['context', 'answer0', 'answer1', 'answer2', 'answer3', 'question']
sciq		['question']
trec		['text']
quac		['context']
health_fact		['main_text', 'claim', 'explanation']
tweet_eval	stance_hillary	['text']
tweet_eval	emoji	['text']
tweet_eval	stance_climate	['text']
tweet_eval	irony	['text']
tweet_eval	stance_abortion	['text']
tweet_eval	stance_feminist	['text']
tweet_eval	hate	['text']
tweet_eval	stance_atheism	['text']
tweet_eval	sentiment	['text']
tweet_eval	offensive	['text']
tweet_eval	emotion	['text']
mocha		['candidate', 'reference', 'question', 'context', 'score']
cc_news		['title', 'description', 'text']
wiki_qa		['question', 'answer']
scitail	snli_format	['sentence1', 'sentence2']
scitail	tsv_format	['premise', 'hypothesis']
crows_pairs		['sent_more', 'sent_less']
kilt_tasks	hotpotqa	['input']
kilt_tasks	nq	['input']
samsum		['dialogue', 'summary']
meta_woz	dialogues	['utterance']
asset	ratings	['original', 'simplification']
asset	simplification	['original']
code_x_glue_tc_text_to_code		['code']
jfleg		['sentence']
amazon_us_reviews	Wireless_v1_00	['review_body', 'review_headline']
limit		['sentence']
paws-x	en	['sentence1', 'sentence2']
hate_speech18		['text']
xnli	en	['premise', 'hypothesis']
great_code		[]
Zaid	quac_expanded	['context', 'question']
Zaid	coqa_expanded	['story', 'question']
common_gen		['target']
multi_x_science_sum		['abs', 'related_work']
conv_ai_3		['initial_request']
sms_spam		['sms']
ncbi_disease		[]
sent_comp		[]
scientific_papers	arxiv	['abstract']
scientific_papers	pubmed	['abstract']
tab_fact	tab_fact	['statement', 'table_text', 'table_caption']
art		['observation_1', 'observation_2']
tydiqa	primary_task	['question_text', 'document_title', 'document_plaintext']
tydiqa	secondary_task	['context', 'question', 'title']
enriched_web_nlg	en	[]
paws	labeled_final	['sentence1', 'sentence2']
paws	labeled_swap	['sentence1', 'sentence2']
paws	unlabeled_final	['sentence1', 'sentence2']
generated_reviews_enth		[]
guardian_authorship	cross_topic_1	['article']
guardian_authorship	cross_topic_7	['article']
guardian_authorship	cross_genre_1	['article']
guardian_authorship	cross_topic_4	['article']
docred		[]
snli		['premise', 'hypothesis']
conv_ai_2		[]
lambada		[]
biosses		['sentence1', 'sentence2']
dbpedia_14		['content']
piqa		['goal', 'sol1', 'sol2']
ropes		['question', 'situation', 'background']
wiki_split		[]
hlgd		['date_a', 'headline_a', 'date_b', 'headline_b']
subjqa	restaurants	['context', 'question']
subjqa	electronics	['context']
subjqa	books	['context', 'question']
subjqa	grocery	['domain', 'question', 'context']
subjqa	movies	['question', 'context']
subjqa	tripadvisor	['context']
quora		[]
discovery	discovery	['sentence1', 'sentence2']
stsb_multi_mt	en	['sentence1', 'sentence2']
cos_e	v1.11	['question', 'answer']
cos_e	v1.0	['question', 'answer']
zest		['context', 'question']
amazon_polarity		['title', 'content']
hotpot_qa	distractor	['answer', 'question']
hotpot_qa	fullwiki	['question', 'answer']
trivia_qa	unfiltered	['question']
ade_corpus_v2	Ade_corpus_v2_drug_dosage_relation	['text', 'dosage', 'drug']
ade_corpus_v2	Ade_corpus_v2_drug_ade_relation	['text', 'effect', 'drug']
ade_corpus_v2	Ade_corpus_v2_classification	['text']
head_qa	en	['category', 'qtext']
craffel	openai_lambada	[]
scicite		['string', 'sectionName']
scan	filler_num1	['commands', 'actions']
scan	filler_num0	['actions', 'commands']
scan	length	['commands', 'actions']
scan	template_right	['commands', 'actions']
scan	simple	['actions', 'commands']
scan	filler_num2	['commands', 'actions']
scan	template_opposite_right	['commands', 'actions']
scan	filler_num3	['commands', 'actions']
scan	template_around_right	['commands', 'actions']
scan	addprim_turn_left	['commands', 'actions']
scan	template_jump_around_right	['commands', 'actions']
scan	addprim_jump	['commands', 'actions']
amazon_reviews_multi	en	['review_body', 'review_title']
nlu_evaluation_data		['text']
xquad	xquad.en	['context', 'question']
cnn_dailymail	3.0.0	['article', 'highlights']
ai2_arc	ARC-Challenge	['question']
ai2_arc	ARC-Easy	['question', 'letter', 't', 'answerKey']
quarel		['question']
gigaword		['document', 'summary']
xsum		['document', 'summary']
hans		['premise', 'hypothesis']
selqa	answer_selection_analysis	['question']
yahoo_answers_qa		['question']
yelp_review_full		['text']
species_800		[]
winogrande	winogrande_s	['sentence', 'option1', 'option2', 'option1', 'option2']
winogrande	winogrande_xl	['sentence', 'option1', 'option2', 'option1', 'option2']
winogrande	winogrande_l	['option1', 'sentence']
winogrande	winogrande_m	['sentence', 'option1', 'option2', 'option1', 'option2']
winogrande	winogrande_debiased	['sentence', 'option1', 'option2', 'option1', 'option2']
winogrande	winogrande_xs	['sentence', 'option1', 'option2', 'option1', 'option2']
liar		['statement', 'party_affiliation']
onestop_english		['text']
poem_sentiment		['verse_text']
bing_coronavirus_query_set		['Date', 'Query', 'Country']
fever	v2.0	['claim']
fever	v1.0	['claim']
banking77		['text']
scitldr	Abstract	[]
ambig_qa	light	['question', 'selected_answer', 'selected_question']
lama	trex	['masked_sentence', 'obj_label']
wiki_bio		['target_text']
openbookqa	additional	['question_stem']
openbookqa	main	['question_stem']
asnq		['question', 'sentence']
social_i_qa		['context', 'question']
drop		['question', 'passage']
movie_rationales		['review']
web_questions		['question']
multi_nli		['premise', 'hypothesis']
cord19	metadata	['abstract', 'title']
medical_questions_pairs		['question_1', 'question_2']
newspop		['title', 'headline']
jigsaw_unintended_bias		['comment_text']
mc_taco		['sentence', 'question', 'answer']
financial_phrasebank	sentences_allagree	['sentence']
discofuse	discofuse-sport	['coherent_first_sentence', 'incoherent_first_sentence', 'incoherent_second_sentence']
discofuse	discofuse-wikipedia	['incoherent_first_sentence', 'incoherent_second_sentence', 'coherent_first_sentence', 'coherent_second_sentence']
ag_news		['text']
qasc		[]
medal		['text']
wiki_hop	original	['support', 'answer']
wiki_hop	masked	['answer']
sst	default	['sentence']
qa_zre		['question', 'relation']
gutenberg_time		['tok_context', 'time_phrase']
quoref		['question', 'context']
narrativeqa		[]
squad_v2		['context', 'question']
squad		['context', 'question']
xquad_r	en	['context', 'question']
wiqa		[]
humicroedit	subtask-1	['edit']
humicroedit	subtask-2	['edit1', 'edit2', 'edit1', 'edit2']
circa		['question_X', 'canquestion_X']
openai_humaneval		['prompt', 'canonical_solution']
imdb		['text']
race	middle	['article', 'question', 'answer']
race	high	['article', 'question']
race	all	['article', 'question', 'answer']
coqa		['story']
e2e_nlg_cleaned		['key', 'value', 'human_reference']
covid_qa_castorini		['question_query']
mwsc		['answer']
app_reviews		['review']
hyperpartisan_news_detection	byarticle	['text']
hyperpartisan_news_detection	bypublisher	['text']
sem_eval_2010_task_8		['sentence']
nq_open		['question']
blbooksgenre	title_genre_classifiction	['title']
codah	fold_4	['question_propmt', 'candidate']
codah	fold_3	['question_propmt', 'candidate']
codah	fold_2	['question_propmt', 'candidate']
codah	fold_0	['question_propmt', 'candidate']
codah	fold_1	['question_propmt', 'candidate']
codah	codah	['question_propmt', 'candidate']
squadshifts	amazon	['context', 'question']
squadshifts	nyt	['question', 'context']
squadshifts	new_wiki	['question', 'context']
conv_ai		['context']
ecthr_cases	alleged-violation-prediction	[]
swag	regular	['sent2', 'sent1']
qed		['paragraph_text', 'title_text']
quartz		['para']
sem_eval_2014_task_1		['premise', 'hypothesis']
google_wellformed_query		['content']
hellaswag		[]
duorc	SelfRC	['plot', 'question']
duorc	ParaphraseRC	['question', 'plot']
climate_fever		['claim']
math_dataset	algebra__linear_1d_composed	['answer']
math_dataset	algebra__linear_2d	['question', 'answer']
math_dataset	algebra__linear_1d	['question', 'answer']
math_dataset	algebra__linear_2d_composed	['question', 'answer']
multi_news		['doc']
yahoo_answers_topics		['best_answer', 'question_title']
turk		['original']
evidence_infer_treatment	1.1	[]
evidence_infer_treatment	2.0	[]
aeslc		['email_body', 'subject_line']
dream		[]
riddle_sense		['question']
pubmed_qa	pqa_labeled	['question']
wino_bias	type2_pro	['pronoun', 'pronoun', 'referent']
wino_bias	type2_anti	['pronoun', 'referent']
wino_bias	type1_pro	['pronoun', 'referent']
wino_bias	type1_anti	['pronoun', 'referent']
qa_srl		['sentence']
quail		['context', 'question']
tmu_gfm_dataset		['source', 'output']
squad_adversarial	AddSent	['context', 'question']
neural_code_search	evaluation_dataset	['question', 'answer']
emotion		['text']
freebase_qa		['RawQuestion']
story_cloze	2016	['input_sentence_1', 'input_sentence_2', 'input_sentence_3', 'input_sentence_4']
billsum		['title', 'summary', 'text']
esnli		['premise', 'hypothesis']
commonsense_qa		['question']
adversarial_qa	dbert	['context', 'question']
adversarial_qa	dbidaf	['question', 'context']
adversarial_qa	adversarialQA	['context', 'question']
adversarial_qa	droberta	['context', 'question']
acronym_identification		['random_abbr']
